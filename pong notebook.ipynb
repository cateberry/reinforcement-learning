{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pooong.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAIOOkk2L5qx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import datetime\n",
        "import pickle\n",
        "\n",
        "from copy import copy\n",
        "from itertools import count\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# %matplotlib inline\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJHw_5OLM2_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oftgcSXGM2aU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wandb\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY_-vu-0L8jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Preprocess(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Apply preprocessing steps as defined in original DeepMind Atari paper:\n",
        "        https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf\n",
        "        \"\"\"\n",
        "        super(Preprocess,self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=255, shape=(84,84,1), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        new_obs = obs.transpose((2, 0, 1))\n",
        "        new_obs = np.ascontiguousarray(obs, dtype=np.float32) / 255\n",
        "        new_obs = torch.from_numpy(new_obs)\n",
        "\n",
        "        preprocess = transforms.Compose([transforms.ToPILImage(),\n",
        "                                         transforms.Resize((84,84)),          # Resize;            size: (3,84,84)\n",
        "                                         transforms.Grayscale(),           # Apply greyscale;   size: (1,84,84)\n",
        "                                         transforms.ToTensor()])           # Make into tensor\n",
        "\n",
        "        new_obs = preprocess(obs).to(device)\n",
        "        return new_obs\n",
        "\n",
        "\n",
        "class MoveImgChannel(gym.ObservationWrapper):\n",
        "    def __init__(self,env):\n",
        "        super(MoveImgChannel,self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(low=0.0,high=1.0,\n",
        "                                              shape=(self.observation_space.shape[-1],\n",
        "                                                     self.observation_space.shape[0],\n",
        "                                                     self.observation_space.shape[1]),\n",
        "                                              dtype=np.float32)\n",
        "    def observation(self,obs):\n",
        "        obs = np.moveaxis(obs.cpu().numpy(),2,0)\n",
        "        return torch.from_numpy(obs)\n",
        "\n",
        "# Stack previous 4 frames using OpenAI Gym wrapper\n",
        "# ADAPTED FROM: OpenAI Baselines https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames using deque.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)      # store previous 4 states in a deque\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=1.0, shape=(4,84,84), dtype=env.observation_space.dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        frames = np.concatenate([s.cpu() for s in self.frames], axis=0)    # concatenate the first frame 4 times for initial state (no other states seen yet)\n",
        "        frames = torch.from_numpy(frames).to(device)\n",
        "        return frames\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        frames = np.concatenate([s.cpu() for s in self.frames], axis=0)\n",
        "        frames = torch.from_numpy(frames)\n",
        "        frames = frames.to(device)\n",
        "        return frames, reward, done, info               # return stack of 4 previous frames instead of just one frame (and other stuff)\n",
        "\n",
        "# Apply classes for preprocessing and frame stacking to create environment\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = Preprocess(env)\n",
        "    env = FrameStack(env, 4)\n",
        "    #env = MoveImgChannel(env)\n",
        "    return env\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTKJqH0oL_Ub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sample some frames to check preprocessing\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(\"PongDeterministic-v4\")\n",
        "observation = env.reset()\n",
        "\n",
        "for i in range(55):\n",
        "    if i > 50:\n",
        "        plt.imshow(observation.cpu().squeeze(0).permute(1, 2, 0).numpy()[:,:,1],cmap=\"gray\")  # interpolation='none')\n",
        "        plt.show()\n",
        "\n",
        "    observation, _, _, _ = env.step(random.randrange(6))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2vZUXa3MBkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Input shape: \", env.observation_space.shape)\n",
        "print(\"Action space size: \", env.action_space.n)\n",
        "print(\"Actions: \", env.get_action_meanings())\n",
        "\n",
        "# Check dimensions of observation are in correct order (batch size, channels (stack of 4 1 channel frames), height, width)\n",
        "print(observation.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cdrUNihMF3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create replay memory object to store a set number of transitions to sample from at each step\n",
        "Transition = namedtuple(\n",
        "    'Transition', ['state', 'action', 'reward', 'next_state', 'terminal'])\n",
        "\n",
        "class ReplayMemory(object):\n",
        "    def __init__(self, capacity):\n",
        "        self.memory_size = capacity\n",
        "        self.memory_pos = 0\n",
        "        self.memory = []\n",
        "\n",
        "    def save_transition(self, transition):\n",
        "        # If memory isn't full yet, just append new memory to end\n",
        "        if len(self.memory) < self.memory_size:\n",
        "            self.memory_pos = len(self.memory)\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            self.memory_pos = (self.memory_pos + 1) % self.memory_size  # get index of next place to store memory (overwrite oldest first)\n",
        "\n",
        "            # Add new transition to memory\n",
        "            self.memory[self.memory_pos] = transition\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        mem_size = len(self.memory)\n",
        "\n",
        "        # Take a random sample of size batch_size from memory (without replacement)\n",
        "        batch_indices = np.random.choice(mem_size, batch_size, replace=False)\n",
        "        batch = [self.memory[idx] for idx in batch_indices]\n",
        "        return batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waxd29fcMH3y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dims, n_actions=6, learning_rate=0.00025):\n",
        "        super(DQN, self).__init__()\n",
        "        self.input_dims = input_dims\n",
        "        self.actions = n_actions\n",
        "        self.lr = learning_rate\n",
        "        # Network from DeepMind paper\n",
        "        self.conv1 = nn.Conv2d(4, 16, 8, stride=4)  # 4 input channels (4 frames of 1 channel images (greyscale))\n",
        "        self.conv2 = nn.Conv2d(16, 32, 4, stride=2)\n",
        "        #self.conv3 = nn.Conv2d(64, 64, 3, stride=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros((1, *input_dims))\n",
        "            x = F.relu(self.conv1(dummy))\n",
        "            x = F.relu(self.conv2(x))\n",
        "            #x = F.relu(self.conv3(x))\n",
        "            s = x.shape\n",
        "            fc1_size = s[1] * s[2] * s[3]\n",
        "\n",
        "        self.fc1 = nn.Linear(fc1_size, 256)\n",
        "        self.fc2 = nn.Linear(256, self.actions)  # 6 possible actions\n",
        "        self.optimiser = optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        #x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)  # flatten before FC layer\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Return output, i.e. Q values of actions for input state (vector length 6)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv4Gbh2YMMmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQNAgent(object):  # object\n",
        "    def __init__(self, input_dims, n_actions=6, capacity=250000, lr=0.0001, gamma=0.99, epsilon=1,\n",
        "                 eps_min=0.1, eps_decay=1e-5, batch_size=32, target_update=1000):\n",
        "        # Defaults are parameters from DeepMind paper\n",
        "        # Size of state and action spaces\n",
        "        self.state_shape = input_dims\n",
        "        self.n_actions = n_actions\n",
        "        self.capacity = capacity\n",
        "        self.actions = [i for i in range(env.action_space.n)]\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.learning_rate = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.eps_min = eps_min\n",
        "        self.eps_decay = eps_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.target_update = target_update  # number of steps before updating the target network weights\n",
        "        self.step_num = 0  # keep track of steps and eps to know when to update target network\n",
        "        self.ep_steps = 0\n",
        "        self.loss = nn.SmoothL1Loss()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emg8G4FpUhoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class History():\n",
        "    def __init__(self, plot_size=300, plot_every=5):\n",
        "        self.plot_size = plot_size\n",
        "        self.episode_durations = deque([], self.plot_size) # steps per episode\n",
        "        self.means = deque([], self.plot_size) # moving average steps per ep\n",
        "        self.episode_loss = deque([], self.plot_size) # average loss per step per ep\n",
        "        self.indexes = deque([], self.plot_size) # index of last 300 episodes \n",
        "        self.step_loss = [] # loss per step\n",
        "        self.step_eps = []\n",
        "        self.peak_reward = 0 # peak number of steps per ep?\n",
        "        self.peak_mean = 0\n",
        "        self.moving_avg = 0 # moving avg of steps per ep\n",
        "        self.step_count = 0\n",
        "        self.total_episode = 0 # total number of eps\n",
        "        self.plot_every = plot_every\n",
        "\n",
        "    def update(self, t, episode_loss):\n",
        "        self.episode_durations.append(t + 1)\n",
        "        self.episode_loss.append(episode_loss / (t + 1))\n",
        "        self.indexes.append(self.total_episode)\n",
        "        if t + 1 > self.peak_reward:\n",
        "            self.peak_reward = t + 1\n",
        "        if len(self.episode_durations) >= 100: # after 100 episodes\n",
        "            self.means.append(sum(list(self.episode_durations)[-100:]) / 100) # mean of last 100 episode durations (steps per ep)\n",
        "        else: # if fewer than 100 eps:\n",
        "            self.moving_avg = self.moving_avg + (t - self.moving_avg) / (self.total_episode + 1)\n",
        "            self.means.append(self.moving_avg)\n",
        "        if self.means[-1] > self.peak_mean:\n",
        "            self.peak_mean = self.means[-1] # peak avg steps per ep\n",
        "\n",
        "        if self.total_episode % self.plot_every == 0:\n",
        "            self.plot()\n",
        "\n",
        "    def plot(self):\n",
        "        display.clear_output(wait=True)\n",
        "\n",
        "        f, (ax1, ax3) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        ax1.plot(self.indexes, self.episode_durations) # epsiode x steps per ep\n",
        "        ax1.plot(self.indexes, self.means) # episode x moving avg steps per ep\n",
        "        ax1.set_xlabel(\"episode\")\n",
        "        ax1.axhline(self.peak_reward, color='g') # peak number steps per ep \n",
        "        ax1.axhline(self.peak_mean, color='g') # peak moving avg steps per ep\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        ax2.plot(self.indexes, self.episode_loss, 'r') # episodes x avg loss per ep\n",
        "\n",
        "        ax4 = ax3.twinx()\n",
        "        total_step = len(self.step_loss)\n",
        "        sample_rate = total_step // self.plot_size if total_step > (\n",
        "            self.plot_size * 10) else 1\n",
        "        ax3.set_title('total number of steps: {0}'.format(total_step))\n",
        "        ax3.plot(self.step_eps[::sample_rate], 'g') # total steps x epsilon per step\n",
        "        ax4.plot(self.step_loss[::sample_rate], 'b') # total steps x loss per step\n",
        "\n",
        "        plt.pause(0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gU_lOPnMOZT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_action(state, policy_net):\n",
        "    rd = random.random()\n",
        "    if rd < agent.epsilon:  # explore\n",
        "        action = random.randrange(agent.n_actions)\n",
        "    else:\n",
        "        # exploit\n",
        "        with torch.no_grad():\n",
        "            action = policy_net(state.unsqueeze(0)).max(1)[1].item()\n",
        "    return action\n",
        "\n",
        "def train_agent(policy, target, memory):\n",
        "    # Need at least batch_size transitions stored in memory before sampling from memory\n",
        "    if len(memory) < agent.batch_size:\n",
        "        return 0\n",
        "\n",
        "    # Check if target network weights should be updated this step\n",
        "    if agent.step_num % agent.target_update == 0:\n",
        "        target.train()\n",
        "        target.load_state_dict(policy.state_dict())\n",
        "\n",
        "    # Sample batch of memories\n",
        "    transitions = memory.sample(agent.batch_size)\n",
        "    states = torch.stack(([t.state for t in transitions])).to(device)\n",
        "    rewards = torch.stack(([t.reward for t in transitions])).to(device)\n",
        "    next_states = torch.stack(([t.next_state for t in transitions])).to(device)\n",
        "    actions = torch.stack(([torch.LongTensor([t.action]) for t in transitions])).to(device)\n",
        "    mask = torch.stack([torch.Tensor([0]) if t.terminal else torch.Tensor([1]) for t in transitions]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        q_next = target(next_states).max(1)[0].detach() # (32,1)\n",
        "\n",
        "    policy.optimiser.zero_grad()\n",
        "\n",
        "    q_current = policy(states) # (32,6)\n",
        "    actions_onehot = F.one_hot(actions, agent.n_actions).squeeze(1).to(device) # (32,6)\n",
        "    q_current_acts = torch.sum(q_current * actions_onehot, -1).view(agent.batch_size,1) # (32,1)\n",
        "    q_targets = torch.zeros((agent.batch_size,1)).to(device) # (32,1)\n",
        "    q_targets = q_next * mask[:,0]   # non-terminal states take max next Q-value from target model\n",
        "    q_target = q_targets.view(agent.batch_size,1) * agent.gamma + rewards # (32,1)\n",
        "\n",
        "    # Perform a single gradient update just for batch in consideration\n",
        "    policy_net.train()\n",
        "    loss_fn = agent.loss # Huber loss\n",
        "    loss = loss_fn(q_current_acts, q_target)\n",
        "    loss.backward()  # backprop loss\n",
        "    policy.optimiser.step()  # perform gradient update\n",
        "\n",
        "    return loss.detach().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wi2nppjPMUnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32 # 32\n",
        "learning_rate = 0.0001 # 0.0001\n",
        "gamma = 0.99\n",
        "eps_min = 0.02 # 0.05 (best) | try next: 0.02\n",
        "eps_decay = 0.99999\n",
        "capacity = 100000 #100000 (best) | try next: 25000\n",
        "target_update = 2500 #5000 (best) | try next: 1000\n",
        "num_eps = 400\n",
        "steps_before_train = 10 # don't start training right from beginning of epoch\n",
        "\n",
        "# Config for wandb\n",
        "wandb.init(project=\"reinforcement-learning-coursework\", name=\"dqn-pong\",\n",
        "           config={\"batch size\":batch_size, \"learning_rate\": learning_rate, \"eps_min\": eps_min,\n",
        "                   \"eps_decay\": eps_decay,\"capacity\": capacity,\n",
        "                   \"target_update\": target_update})\n",
        "\n",
        "env = make_env(\"PongDeterministic-v4\")\n",
        "observation = env.reset()\n",
        "input_shape = np.array(observation.shape)\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "agent = DQNAgent(input_shape, n_actions, lr=learning_rate, capacity=capacity, gamma=gamma, eps_min=eps_min, eps_decay=eps_decay,\n",
        "                 batch_size=batch_size, target_update=target_update)  # other params default\n",
        "policy_net = DQN(input_shape, n_actions, learning_rate).to(device)\n",
        "target_net = DQN(input_shape, n_actions, learning_rate).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "memory = ReplayMemory(capacity)\n",
        "history = History()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AyBrLslXbGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If loading checkpoint:\n",
        "# resume = False\n",
        "# model_name = 'Basic_DQN' + '_40_200503-1956'# fill in file path for checkpoint\n",
        "# save_name = 'checkpoints/' + model_name\n",
        "\n",
        "# if resume:\n",
        "#     with open(save_name + '.pickle', 'rb') as f:\n",
        "#         data = pickle.load(f)\n",
        "#         history = data['history']\n",
        "#         agent = data['agent']\n",
        "#         memory = data['memory']\n",
        "#     checkpoint = torch.load(save_name + '.pt')\n",
        "#     policy_net = DQN(input_shape, n_actions, learning_rate).to(device)\n",
        "#     target_net = DQN(input_shape, n_actions, learning_rate).to(device)\n",
        "#     policy_net.load_state_dict(checkpoint['policy_net'])\n",
        "#     target_net.load_state_dict(checkpoint['target_net'])\n",
        "#     policy_net.optimiser.load_state_dict(checkpoint['optimiser'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z2NjabAMQbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_net.eval() # so that weights aren't updated\n",
        "scores, step_eps, avg_losses = [], [], []\n",
        "\n",
        "for e in range(num_eps):\n",
        "    score = 0\n",
        "    avg_loss = 0\n",
        "    agent.ep_steps = 0\n",
        "    history.total_episode += 1\n",
        "\n",
        "    # Take initial state as stack of first frame 4 times\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in count():\n",
        "        agent.step_num += 1\n",
        "        agent.ep_steps += 1\n",
        "        history.step_count += 1\n",
        "        # Take an action (epsilon hyperparameters are defined when creating agent)\n",
        "        action = next_action(state, policy_net)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        score += reward\n",
        "\n",
        "        # Save transition to memory\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        memory.save_transition(Transition(state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        # Sample batch of memories and update weights\n",
        "        if agent.ep_steps > steps_before_train:\n",
        "            loss = train_agent(policy_net, target_net, memory)\n",
        "            avg_loss += loss\n",
        "            history.step_loss.append(loss)\n",
        "            wandb.log({'loss': loss, 'eps': agent.epsilon}, step=agent.step_num)\n",
        "\n",
        "        history.step_eps.append(agent.epsilon)\n",
        "        agent.epsilon = agent.eps_decay ** agent.step_num if \\\n",
        "         agent.epsilon > agent.eps_min else agent.eps_min\n",
        "\n",
        "        if done:\n",
        "            history.update(t, avg_loss)\n",
        "            break\n",
        "        # raise Exception()\n",
        "    step_eps.append(agent.epsilon)  # save epsilon value of episode\n",
        "    scores.append(score)\n",
        "    avg_losses.append(avg_loss / (t + 1))\n",
        "    avg_score = np.mean(scores[-100:])\n",
        "    x = [i + 1 for i in range(num_eps)]\n",
        "    wandb.log({'avg reward': avg_score},step=agent.step_num)\n",
        "    # plotLearning(x, scores, step_eps)\n",
        "\n",
        "    print(\"Episode: \", e + 1, \" | Score: %.2f\" % score, \" | Average score: %.2f\" % avg_score,\n",
        "          \" | Episode Loss: %.6f\" % avg_losses[-1],\n",
        "          \" | Epsilon: %.4f\" % agent.epsilon, \" | Steps: %i\" % agent.step_num)\n",
        "\n",
        "    # Store state checkpoints every 40 epochs\n",
        "    # if e % 40 == 0:\n",
        "    #     model_name = 'Basic_DQN_' + str(e) + \"_\" + str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\"))\n",
        "    #     save_name = '/content/drive/My Drive/Colab Notebooks/checkpoints/' + model_name\n",
        "\n",
        "    #     torch.save({\n",
        "    #         'policy_net': policy_net.state_dict(),\n",
        "    #         'target_net': target_net.state_dict(),\n",
        "    #         'optimiser': policy_net.optimiser.state_dict()\n",
        "    #     }, save_name + '.pt')\n",
        "\n",
        "    #     with open(save_name + '.pickle', 'wb') as f:\n",
        "    #         pickle.dump({'agent': agent, 'history': history, 'agent': agent},\n",
        "    #                     f, pickle.HIGHEST_PROTOCOL)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Vy08q12zTI2",
        "colab_type": "text"
      },
      "source": [
        "An episode in Pong runs until one of the players reaches a score of 21.\n",
        "\n",
        "Rounds in Pong correspond to one of the players getting a score & reward of 1 and winning a rally, and the losing player getting a score of 0 and a reward of -1.\n",
        "\n",
        "The running mean score per episode, over the trailing 100 episodes, at the point I stopped training was 2.5, i.e. the trained AI Agent would win each episode 21 points to 18.5. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-2jC8v3MVfC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_name_done = 'checkpoints/' + model_name + \"_done_\" + str(datetime.datetime.now().strftime(\"%y%m%d-%H%M\"))\n",
        "torch.save({\n",
        "    'policy_net': policy_net.state_dict(),\n",
        "    'target_net': target_net.state_dict(),\n",
        "    'optimiser': policy_net.optimiser.state_dict()\n",
        "}, save_name_done + '.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
